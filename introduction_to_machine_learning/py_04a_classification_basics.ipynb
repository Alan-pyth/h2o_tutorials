{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with H2O - Tutorial 4a: Classification Models (Basics)\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Objective**:\n",
    "\n",
    "- This tutorial explains how to build classification models with four different H2O algorithms.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Titanic Dataset:**\n",
    "\n",
    "- Source: https://www.kaggle.com/c/titanic/data\n",
    "\n",
    "<hr>\n",
    "    \n",
    "**Algorithms**:\n",
    "\n",
    "1. GLM\n",
    "2. DRF\n",
    "3. GBM\n",
    "4. DNN\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Full Technical Reference:**\n",
    "\n",
    "- http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: java version \"1.8.0_121\"; Java(TM) SE Runtime Environment (build 1.8.0_121-b13); Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)\n",
      "  Starting server from /home/joe/anaconda3/lib/python3.5/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpmeub2v9h\n",
      "  JVM stdout: /tmp/tmpmeub2v9h/h2o_joe_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpmeub2v9h/h2o_joe_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.10.4.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>5 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_joe_iqh83j</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>5.210 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.5.2 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster version:        3.10.4.3\n",
       "H2O cluster version age:    5 days\n",
       "H2O cluster name:           H2O_from_python_joe_iqh83j\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    5.210 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "Python version:             3.5.2 final\n",
       "--------------------------  ------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start and connect to a local H2O cluster\n",
    "import h2o\n",
    "h2o.init(nthreads = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  PassengerId</th><th style=\"text-align: right;\">  Survived</th><th style=\"text-align: right;\">  Pclass</th><th>Name                                               </th><th>Sex   </th><th style=\"text-align: right;\">  Age</th><th style=\"text-align: right;\">  SibSp</th><th style=\"text-align: right;\">  Parch</th><th style=\"text-align: right;\">  Ticket</th><th style=\"text-align: right;\">   Fare</th><th>Cabin  </th><th>Embarked  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">       3</td><td>Braund, Mr. Owen Harris                            </td><td>male  </td><td style=\"text-align: right;\">   22</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\"> 7.25  </td><td>       </td><td>S         </td></tr>\n",
       "<tr><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">       1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td style=\"text-align: right;\">   38</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">71.2833</td><td>C85    </td><td>C         </td></tr>\n",
       "<tr><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">       3</td><td>Heikkinen, Miss. Laina                             </td><td>female</td><td style=\"text-align: right;\">   26</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\"> 7.925 </td><td>       </td><td>S         </td></tr>\n",
       "<tr><td style=\"text-align: right;\">            4</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">       1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)       </td><td>female</td><td style=\"text-align: right;\">   35</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">  113803</td><td style=\"text-align: right;\">53.1   </td><td>C123   </td><td>S         </td></tr>\n",
       "<tr><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">       3</td><td>Allen, Mr. William Henry                           </td><td>male  </td><td style=\"text-align: right;\">   35</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">  373450</td><td style=\"text-align: right;\"> 8.05  </td><td>       </td><td>S         </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Titanic data (local CSV)\n",
    "titanic = h2o.import_file(\"kaggle_titanic.csv\")\n",
    "titanic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert 'Survived' and 'Pclass' to categorical values\n",
    "titanic['Survived'] = titanic['Survived'].asfactor()\n",
    "titanic['Pclass'] = titanic['Pclass'].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  Survived</th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">    549</td></tr>\n",
       "<tr><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">    342</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  Pclass</th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">    216</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">    184</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       3</td><td style=\"text-align: right;\">    491</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Pclass'].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Sex   </th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>female</td><td style=\"text-align: right;\">    314</td></tr>\n",
       "<tr><td>male  </td><td style=\"text-align: right;\">    577</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Sex'].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEZCAYAAABiu9n+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREVEqRiIiISikSERFR6f8D9kvgQ+SJpjUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb4c0f19e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic['Age'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb2d778fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic['SibSp'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb56373128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic['Parch'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiKiliSMiIioJQkjIiJqScKIiIha/j8HMjYy5HJCPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb56373a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic['Fare'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Embarked  </th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>C         </td><td style=\"text-align: right;\">    168</td></tr>\n",
       "<tr><td>Q         </td><td style=\"text-align: right;\">     77</td></tr>\n",
       "<tr><td>S         </td><td style=\"text-align: right;\">    644</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Embarked'].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define features (or predictors) manually\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the H2O data frame into training/test sets\n",
    "# so we can evaluate out-of-bag performance\n",
    "titanic_split = titanic.split_frame(ratios = [0.8], seed = 1234)\n",
    "\n",
    "titanic_train = titanic_split[0] # using 80% for training\n",
    "titanic_test = titanic_split[1]  # using the rest 20% for out-of-bag evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Generalized Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Build a Generalized Linear Model (GLM) with default settings\n",
    "\n",
    "# Import the function for GLM\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "\n",
    "# Set up GLM for binary classification\n",
    "glm_default = H2OGeneralizedLinearEstimator(family = 'binomial', model_id = 'glm_default')\n",
    "\n",
    "# Use .train() to build the model\n",
    "glm_default.train(x = features, \n",
    "                  y = 'Survived', \n",
    "                  training_frame = titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGeneralizedLinearEstimator :  Generalized Linear Modeling\n",
      "Model Key:  glm_default\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.1382288206898215\n",
      "RMSE: 0.37179136715343664\n",
      "LogLoss: 0.4365971276608109\n",
      "Null degrees of freedom: 711\n",
      "Residual degrees of freedom: 700\n",
      "Null deviance: 939.9987544934203\n",
      "Residual deviance: 621.7143097889948\n",
      "AIC: 645.7143097889948\n",
      "AUC: 0.8541387024608501\n",
      "Gini: 0.7082774049217002\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.326408717958219: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>354.0</td>\n",
       "<td>93.0</td>\n",
       "<td>0.2081</td>\n",
       "<td> (93.0/447.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>54.0</td>\n",
       "<td>211.0</td>\n",
       "<td>0.2038</td>\n",
       "<td> (54.0/265.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>408.0</td>\n",
       "<td>304.0</td>\n",
       "<td>0.2065</td>\n",
       "<td> (147.0/712.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  -------------\n",
       "0      354  93   0.2081   (93.0/447.0)\n",
       "1      54   211  0.2038   (54.0/265.0)\n",
       "Total  408  304  0.2065   (147.0/712.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3264087</td>\n",
       "<td>0.7416520</td>\n",
       "<td>231.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2662247</td>\n",
       "<td>0.7879656</td>\n",
       "<td>251.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6655020</td>\n",
       "<td>0.8023379</td>\n",
       "<td>122.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6655020</td>\n",
       "<td>0.8146067</td>\n",
       "<td>122.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9710368</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0497483</td>\n",
       "<td>1.0</td>\n",
       "<td>384.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9710368</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.6655020</td>\n",
       "<td>0.6016370</td>\n",
       "<td>122.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.3403702</td>\n",
       "<td>0.7919463</td>\n",
       "<td>230.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.3264087</td>\n",
       "<td>0.7940864</td>\n",
       "<td>231.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.326409     0.741652  231\n",
       "max f2                       0.266225     0.787966  251\n",
       "max f0point5                 0.665502     0.802338  122\n",
       "max accuracy                 0.665502     0.814607  122\n",
       "max precision                0.971037     1         0\n",
       "max recall                   0.0497483    1         384\n",
       "max specificity              0.971037     1         0\n",
       "max absolute_mcc             0.665502     0.601637  122\n",
       "max min_per_class_accuracy   0.34037      0.791946  230\n",
       "max mean_per_class_accuracy  0.326409     0.794086  231"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 37.22 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0112360</td>\n",
       "<td>0.9610609</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0301887</td>\n",
       "<td>0.0301887</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0210674</td>\n",
       "<td>0.9552300</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.0566038</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0308989</td>\n",
       "<td>0.9519189</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.0830189</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0407303</td>\n",
       "<td>0.9421320</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.1094340</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0505618</td>\n",
       "<td>0.9334337</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.1358491</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1011236</td>\n",
       "<td>0.8705719</td>\n",
       "<td>2.5375262</td>\n",
       "<td>2.6121593</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.9722222</td>\n",
       "<td>0.1283019</td>\n",
       "<td>0.2641509</td>\n",
       "<td>153.7526205</td>\n",
       "<td>161.2159329</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1502809</td>\n",
       "<td>0.7799310</td>\n",
       "<td>2.6100270</td>\n",
       "<td>2.6114618</td>\n",
       "<td>0.9714286</td>\n",
       "<td>0.9719626</td>\n",
       "<td>0.1283019</td>\n",
       "<td>0.3924528</td>\n",
       "<td>161.0026954</td>\n",
       "<td>161.1461823</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2008427</td>\n",
       "<td>0.7067333</td>\n",
       "<td>1.9404612</td>\n",
       "<td>2.4425386</td>\n",
       "<td>0.7222222</td>\n",
       "<td>0.9090909</td>\n",
       "<td>0.0981132</td>\n",
       "<td>0.4905660</td>\n",
       "<td>94.0461216</td>\n",
       "<td>144.2538593</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3005618</td>\n",
       "<td>0.5934800</td>\n",
       "<td>1.5515280</td>\n",
       "<td>2.1469229</td>\n",
       "<td>0.5774648</td>\n",
       "<td>0.7990654</td>\n",
       "<td>0.1547170</td>\n",
       "<td>0.6452830</td>\n",
       "<td>55.1528036</td>\n",
       "<td>114.6922941</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4002809</td>\n",
       "<td>0.4039384</td>\n",
       "<td>1.1731066</td>\n",
       "<td>1.9043231</td>\n",
       "<td>0.4366197</td>\n",
       "<td>0.7087719</td>\n",
       "<td>0.1169811</td>\n",
       "<td>0.7622642</td>\n",
       "<td>17.3106564</td>\n",
       "<td>90.4323072</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.2226229</td>\n",
       "<td>0.6811587</td>\n",
       "<td>1.6603774</td>\n",
       "<td>0.2535211</td>\n",
       "<td>0.6179775</td>\n",
       "<td>0.0679245</td>\n",
       "<td>0.8301887</td>\n",
       "<td>-31.8841350</td>\n",
       "<td>66.0377358</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5997191</td>\n",
       "<td>0.1387461</td>\n",
       "<td>0.4919479</td>\n",
       "<td>1.4660952</td>\n",
       "<td>0.1830986</td>\n",
       "<td>0.5456674</td>\n",
       "<td>0.0490566</td>\n",
       "<td>0.8792453</td>\n",
       "<td>-50.8052086</td>\n",
       "<td>46.6095179</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7022472</td>\n",
       "<td>0.1172022</td>\n",
       "<td>0.2944430</td>\n",
       "<td>1.2950340</td>\n",
       "<td>0.1095890</td>\n",
       "<td>0.482</td>\n",
       "<td>0.0301887</td>\n",
       "<td>0.9094340</td>\n",
       "<td>-70.5556991</td>\n",
       "<td>29.5033962</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7991573</td>\n",
       "<td>0.0923965</td>\n",
       "<td>0.3115122</td>\n",
       "<td>1.1757668</td>\n",
       "<td>0.1159420</td>\n",
       "<td>0.4376098</td>\n",
       "<td>0.0301887</td>\n",
       "<td>0.9396226</td>\n",
       "<td>-68.8487832</td>\n",
       "<td>17.5766820</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8988764</td>\n",
       "<td>0.0763351</td>\n",
       "<td>0.4541058</td>\n",
       "<td>1.0957075</td>\n",
       "<td>0.1690141</td>\n",
       "<td>0.4078125</td>\n",
       "<td>0.0452830</td>\n",
       "<td>0.9849057</td>\n",
       "<td>-54.5894233</td>\n",
       "<td>9.5707547</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0101444</td>\n",
       "<td>0.1492662</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0555556</td>\n",
       "<td>0.3721910</td>\n",
       "<td>0.0150943</td>\n",
       "<td>1.0</td>\n",
       "<td>-85.0733753</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.011236                    0.961061           2.68679   2.68679            1                1                           0.0301887       0.0301887                  168.679   168.679\n",
       "    2        0.0210674                   0.95523            2.68679   2.68679            1                1                           0.0264151       0.0566038                  168.679   168.679\n",
       "    3        0.0308989                   0.951919           2.68679   2.68679            1                1                           0.0264151       0.0830189                  168.679   168.679\n",
       "    4        0.0407303                   0.942132           2.68679   2.68679            1                1                           0.0264151       0.109434                   168.679   168.679\n",
       "    5        0.0505618                   0.933434           2.68679   2.68679            1                1                           0.0264151       0.135849                   168.679   168.679\n",
       "    6        0.101124                    0.870572           2.53753   2.61216            0.944444         0.972222                    0.128302        0.264151                   153.753   161.216\n",
       "    7        0.150281                    0.779931           2.61003   2.61146            0.971429         0.971963                    0.128302        0.392453                   161.003   161.146\n",
       "    8        0.200843                    0.706733           1.94046   2.44254            0.722222         0.909091                    0.0981132       0.490566                   94.0461   144.254\n",
       "    9        0.300562                    0.59348            1.55153   2.14692            0.577465         0.799065                    0.154717        0.645283                   55.1528   114.692\n",
       "    10       0.400281                    0.403938           1.17311   1.90432            0.43662          0.708772                    0.116981        0.762264                   17.3107   90.4323\n",
       "    11       0.5                         0.222623           0.681159  1.66038            0.253521         0.617978                    0.0679245       0.830189                   -31.8841  66.0377\n",
       "    12       0.599719                    0.138746           0.491948  1.4661             0.183099         0.545667                    0.0490566       0.879245                   -50.8052  46.6095\n",
       "    13       0.702247                    0.117202           0.294443  1.29503            0.109589         0.482                       0.0301887       0.909434                   -70.5557  29.5034\n",
       "    14       0.799157                    0.0923965          0.311512  1.17577            0.115942         0.43761                     0.0301887       0.939623                   -68.8488  17.5767\n",
       "    15       0.898876                    0.0763351          0.454106  1.09571            0.169014         0.407813                    0.045283        0.984906                   -54.5894  9.57075\n",
       "    16       1                           0.0101444          0.149266  1                  0.0555556        0.372191                    0.0150943       1                          -85.0734  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>iteration</b></td>\n",
       "<td><b>negative_log_likelihood</b></td>\n",
       "<td><b>objective</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:32</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>0</td>\n",
       "<td>469.9993772</td>\n",
       "<td>0.6601115</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:32</td>\n",
       "<td> 0.024 sec</td>\n",
       "<td>1</td>\n",
       "<td>319.4025691</td>\n",
       "<td>0.4494054</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:32</td>\n",
       "<td> 0.030 sec</td>\n",
       "<td>2</td>\n",
       "<td>311.1902527</td>\n",
       "<td>0.4382131</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:32</td>\n",
       "<td> 0.032 sec</td>\n",
       "<td>3</td>\n",
       "<td>310.8603386</td>\n",
       "<td>0.4378358</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:32</td>\n",
       "<td> 0.034 sec</td>\n",
       "<td>4</td>\n",
       "<td>310.8571549</td>\n",
       "<td>0.4378367</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    iteration    negative_log_likelihood    objective\n",
       "--  -------------------  ----------  -----------  -------------------------  -----------\n",
       "    2017-04-06 14:32:32  0.000 sec   0            469.999                    0.660111\n",
       "    2017-04-06 14:32:32  0.024 sec   1            319.403                    0.449405\n",
       "    2017-04-06 14:32:32  0.030 sec   2            311.19                     0.438213\n",
       "    2017-04-06 14:32:32  0.032 sec   3            310.86                     0.437836\n",
       "    2017-04-06 14:32:32  0.034 sec   4            310.857                    0.437837"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model performance on training dataset\n",
    "glm_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.14604424271085578\n",
      "RMSE: 0.38215735333872064\n",
      "LogLoss: 0.46125694368411685\n",
      "Null degrees of freedom: 178\n",
      "Residual degrees of freedom: 167\n",
      "Null deviance: 247.17154578244123\n",
      "Residual deviance: 165.12998583891383\n",
      "AIC: 189.12998583891383\n",
      "AUC: 0.8584797555385791\n",
      "Gini: 0.7169595110771583\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.6037543251670895: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>96.0</td>\n",
       "<td>6.0</td>\n",
       "<td>0.0588</td>\n",
       "<td> (6.0/102.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>24.0</td>\n",
       "<td>53.0</td>\n",
       "<td>0.3117</td>\n",
       "<td> (24.0/77.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>120.0</td>\n",
       "<td>59.0</td>\n",
       "<td>0.1676</td>\n",
       "<td> (30.0/179.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  ------------\n",
       "0      96   6    0.0588   (6.0/102.0)\n",
       "1      24   53   0.3117   (24.0/77.0)\n",
       "Total  120  59   0.1676   (30.0/179.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.6037543</td>\n",
       "<td>0.7794118</td>\n",
       "<td>55.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1357796</td>\n",
       "<td>0.8391608</td>\n",
       "<td>113.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6037543</td>\n",
       "<td>0.8466454</td>\n",
       "<td>55.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6037543</td>\n",
       "<td>0.8324022</td>\n",
       "<td>55.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8170662</td>\n",
       "<td>0.9629630</td>\n",
       "<td>26.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0628163</td>\n",
       "<td>1.0</td>\n",
       "<td>153.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9715229</td>\n",
       "<td>0.9901961</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.6037543</td>\n",
       "<td>0.6630044</td>\n",
       "<td>55.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.4656981</td>\n",
       "<td>0.7532468</td>\n",
       "<td>73.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.6037543</td>\n",
       "<td>0.8147441</td>\n",
       "<td>55.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.603754     0.779412  55\n",
       "max f2                       0.13578      0.839161  113\n",
       "max f0point5                 0.603754     0.846645  55\n",
       "max accuracy                 0.603754     0.832402  55\n",
       "max precision                0.817066     0.962963  26\n",
       "max recall                   0.0628163    1         153\n",
       "max specificity              0.971523     0.990196  0\n",
       "max absolute_mcc             0.603754     0.663004  55\n",
       "max min_per_class_accuracy   0.465698     0.753247  73\n",
       "max mean_per_class_accuracy  0.603754     0.814744  55"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 43.02 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0111732</td>\n",
       "<td>0.9640618</td>\n",
       "<td>1.1623377</td>\n",
       "<td>1.1623377</td>\n",
       "<td>0.5</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.0129870</td>\n",
       "<td>16.2337662</td>\n",
       "<td>16.2337662</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0223464</td>\n",
       "<td>0.9548586</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.7435065</td>\n",
       "<td>1.0</td>\n",
       "<td>0.75</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0389610</td>\n",
       "<td>132.4675325</td>\n",
       "<td>74.3506494</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0335196</td>\n",
       "<td>0.9467517</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.9372294</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8333333</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0649351</td>\n",
       "<td>132.4675325</td>\n",
       "<td>93.7229437</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0446927</td>\n",
       "<td>0.9327764</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.0340909</td>\n",
       "<td>1.0</td>\n",
       "<td>0.875</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0909091</td>\n",
       "<td>132.4675325</td>\n",
       "<td>103.4090909</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0502793</td>\n",
       "<td>0.9178485</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.0663781</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.1038961</td>\n",
       "<td>132.4675325</td>\n",
       "<td>106.6378066</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1005587</td>\n",
       "<td>0.8898986</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.1955267</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.1168831</td>\n",
       "<td>0.2207792</td>\n",
       "<td>132.4675325</td>\n",
       "<td>119.5526696</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1508380</td>\n",
       "<td>0.8139070</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.2385762</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9629630</td>\n",
       "<td>0.1168831</td>\n",
       "<td>0.3376623</td>\n",
       "<td>132.4675325</td>\n",
       "<td>123.8576239</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2011173</td>\n",
       "<td>0.7735290</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.1955267</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.4415584</td>\n",
       "<td>106.6378066</td>\n",
       "<td>119.5526696</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3016760</td>\n",
       "<td>0.6387205</td>\n",
       "<td>1.8080808</td>\n",
       "<td>2.0663781</td>\n",
       "<td>0.7777778</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.1818182</td>\n",
       "<td>0.6233766</td>\n",
       "<td>80.8080808</td>\n",
       "<td>106.6378066</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4022346</td>\n",
       "<td>0.4916867</td>\n",
       "<td>1.0331890</td>\n",
       "<td>1.8080808</td>\n",
       "<td>0.4444444</td>\n",
       "<td>0.7777778</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.7272727</td>\n",
       "<td>3.3189033</td>\n",
       "<td>80.8080808</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5027933</td>\n",
       "<td>0.3361618</td>\n",
       "<td>0.7748918</td>\n",
       "<td>1.6014430</td>\n",
       "<td>0.3333333</td>\n",
       "<td>0.6888889</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.8051948</td>\n",
       "<td>-22.5108225</td>\n",
       "<td>60.1443001</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5977654</td>\n",
       "<td>0.2088974</td>\n",
       "<td>0.8204736</td>\n",
       "<td>1.4773638</td>\n",
       "<td>0.3529412</td>\n",
       "<td>0.6355140</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.8831169</td>\n",
       "<td>-17.9526356</td>\n",
       "<td>47.7363758</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6983240</td>\n",
       "<td>0.1276819</td>\n",
       "<td>0.5165945</td>\n",
       "<td>1.3390130</td>\n",
       "<td>0.2222222</td>\n",
       "<td>0.576</td>\n",
       "<td>0.0519481</td>\n",
       "<td>0.9350649</td>\n",
       "<td>-48.3405483</td>\n",
       "<td>33.9012987</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7988827</td>\n",
       "<td>0.1089369</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1704659</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5034965</td>\n",
       "<td>0.0</td>\n",
       "<td>0.9350649</td>\n",
       "<td>-100.0</td>\n",
       "<td>17.0465898</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8994413</td>\n",
       "<td>0.0820554</td>\n",
       "<td>0.3874459</td>\n",
       "<td>1.0829233</td>\n",
       "<td>0.1666667</td>\n",
       "<td>0.4658385</td>\n",
       "<td>0.0389610</td>\n",
       "<td>0.9740260</td>\n",
       "<td>-61.2554113</td>\n",
       "<td>8.2923288</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0302597</td>\n",
       "<td>0.2582973</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1111111</td>\n",
       "<td>0.4301676</td>\n",
       "<td>0.0259740</td>\n",
       "<td>1.0</td>\n",
       "<td>-74.1702742</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0111732                   0.964062           1.16234   1.16234            0.5              0.5                         0.012987        0.012987                   16.2338   16.2338\n",
       "    2        0.0223464                   0.954859           2.32468   1.74351            1                0.75                        0.025974        0.038961                   132.468   74.3506\n",
       "    3        0.0335196                   0.946752           2.32468   1.93723            1                0.833333                    0.025974        0.0649351                  132.468   93.7229\n",
       "    4        0.0446927                   0.932776           2.32468   2.03409            1                0.875                       0.025974        0.0909091                  132.468   103.409\n",
       "    5        0.0502793                   0.917848           2.32468   2.06638            1                0.888889                    0.012987        0.103896                   132.468   106.638\n",
       "    6        0.100559                    0.889899           2.32468   2.19553            1                0.944444                    0.116883        0.220779                   132.468   119.553\n",
       "    7        0.150838                    0.813907           2.32468   2.23858            1                0.962963                    0.116883        0.337662                   132.468   123.858\n",
       "    8        0.201117                    0.773529           2.06638   2.19553            0.888889         0.944444                    0.103896        0.441558                   106.638   119.553\n",
       "    9        0.301676                    0.638721           1.80808   2.06638            0.777778         0.888889                    0.181818        0.623377                   80.8081   106.638\n",
       "    10       0.402235                    0.491687           1.03319   1.80808            0.444444         0.777778                    0.103896        0.727273                   3.3189    80.8081\n",
       "    11       0.502793                    0.336162           0.774892  1.60144            0.333333         0.688889                    0.0779221       0.805195                   -22.5108  60.1443\n",
       "    12       0.597765                    0.208897           0.820474  1.47736            0.352941         0.635514                    0.0779221       0.883117                   -17.9526  47.7364\n",
       "    13       0.698324                    0.127682           0.516595  1.33901            0.222222         0.576                       0.0519481       0.935065                   -48.3405  33.9013\n",
       "    14       0.798883                    0.108937           0         1.17047            0                0.503497                    0               0.935065                   -100      17.0466\n",
       "    15       0.899441                    0.0820554          0.387446  1.08292            0.166667         0.465839                    0.038961        0.974026                   -61.2554  8.29233\n",
       "    16       1                           0.0302597          0.258297  1                  0.111111         0.430168                    0.025974        1                          -74.1703  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model performance on test dataset\n",
    "glm_default.model_performance(titanic_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "\n",
    "## Distributed Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Build a Distributed Random Forest (DRF) model with default settings\n",
    "\n",
    "# Import the function for DRF\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "\n",
    "# Set up DRF for regression\n",
    "# Add a seed for reproducibility\n",
    "drf_default = H2ORandomForestEstimator(model_id = 'drf_default', seed = 1234)\n",
    "\n",
    "# Use .train() to build the model\n",
    "drf_default.train(x = features, \n",
    "                  y = 'Survived', \n",
    "                  training_frame = titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2ORandomForestEstimator :  Distributed Random Forest\n",
      "Model Key:  drf_default\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: drf\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.1369317850631112\n",
      "RMSE: 0.37004295029511264\n",
      "LogLoss: 0.4610865952257547\n",
      "Mean Per-Class Error: 0.191427124224389\n",
      "AUC: 0.8571567261829387\n",
      "Gini: 0.7143134523658774\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5226372166683799: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>399.0</td>\n",
       "<td>48.0</td>\n",
       "<td>0.1074</td>\n",
       "<td> (48.0/447.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>73.0</td>\n",
       "<td>192.0</td>\n",
       "<td>0.2755</td>\n",
       "<td> (73.0/265.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>472.0</td>\n",
       "<td>240.0</td>\n",
       "<td>0.1699</td>\n",
       "<td> (121.0/712.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  -------------\n",
       "0      399  48   0.1074   (48.0/447.0)\n",
       "1      73   192  0.2755   (73.0/265.0)\n",
       "Total  472  240  0.1699   (121.0/712.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.5226372</td>\n",
       "<td>0.7603960</td>\n",
       "<td>149.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2209992</td>\n",
       "<td>0.7898399</td>\n",
       "<td>247.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6807123</td>\n",
       "<td>0.8059701</td>\n",
       "<td>108.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5369729</td>\n",
       "<td>0.8300562</td>\n",
       "<td>145.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0033102</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5226372</td>\n",
       "<td>0.6310849</td>\n",
       "<td>149.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.3439932</td>\n",
       "<td>0.7829978</td>\n",
       "<td>198.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5226372</td>\n",
       "<td>0.8085729</td>\n",
       "<td>149.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.522637     0.760396  149\n",
       "max f2                       0.220999     0.78984   247\n",
       "max f0point5                 0.680712     0.80597   108\n",
       "max accuracy                 0.536973     0.830056  145\n",
       "max precision                1            1         0\n",
       "max recall                   0.00331015   1         398\n",
       "max specificity              1            1         0\n",
       "max absolute_mcc             0.522637     0.631085  149\n",
       "max min_per_class_accuracy   0.343993     0.782998  198\n",
       "max mean_per_class_accuracy  0.522637     0.808573  149"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 37.22 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0449438</td>\n",
       "<td>0.9995745</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1207547</td>\n",
       "<td>0.1207547</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0575843</td>\n",
       "<td>0.9981459</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0339623</td>\n",
       "<td>0.1547170</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.1011236</td>\n",
       "<td>0.9669435</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1169811</td>\n",
       "<td>0.2716981</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.1502809</td>\n",
       "<td>0.9195929</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1320755</td>\n",
       "<td>0.4037736</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.2008427</td>\n",
       "<td>0.8723681</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1358491</td>\n",
       "<td>0.5396226</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.3005618</td>\n",
       "<td>0.6495294</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2679245</td>\n",
       "<td>0.8075472</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.4002809</td>\n",
       "<td>0.3332100</td>\n",
       "<td>1.7028966</td>\n",
       "<td>2.4416816</td>\n",
       "<td>0.6338028</td>\n",
       "<td>0.9087719</td>\n",
       "<td>0.1698113</td>\n",
       "<td>0.9773585</td>\n",
       "<td>70.2896625</td>\n",
       "<td>144.1681562</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1736858</td>\n",
       "<td>0.1513686</td>\n",
       "<td>1.9849057</td>\n",
       "<td>0.0563380</td>\n",
       "<td>0.7387640</td>\n",
       "<td>0.0150943</td>\n",
       "<td>0.9924528</td>\n",
       "<td>-84.8631411</td>\n",
       "<td>98.4905660</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.6095506</td>\n",
       "<td>0.1077212</td>\n",
       "<td>0.0688921</td>\n",
       "<td>1.6405530</td>\n",
       "<td>0.0256410</td>\n",
       "<td>0.6105991</td>\n",
       "<td>0.0075472</td>\n",
       "<td>1.0</td>\n",
       "<td>-93.1107886</td>\n",
       "<td>64.0552995</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.6994382</td>\n",
       "<td>0.0658923</td>\n",
       "<td>0.0</td>\n",
       "<td>1.4297189</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5321285</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>42.9718876</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.7991573</td>\n",
       "<td>0.0438331</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2513181</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4657293</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.1318102</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.8988764</td>\n",
       "<td>0.0221624</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1125</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4140625</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.2500000</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.3721910</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0449438                   0.999574           2.68679    2.68679            1                1                           0.120755        0.120755                   168.679   168.679\n",
       "    2        0.0575843                   0.998146           2.68679    2.68679            1                1                           0.0339623       0.154717                   168.679   168.679\n",
       "    3        0.101124                    0.966944           2.68679    2.68679            1                1                           0.116981        0.271698                   168.679   168.679\n",
       "    4        0.150281                    0.919593           2.68679    2.68679            1                1                           0.132075        0.403774                   168.679   168.679\n",
       "    5        0.200843                    0.872368           2.68679    2.68679            1                1                           0.135849        0.539623                   168.679   168.679\n",
       "    6        0.300562                    0.649529           2.68679    2.68679            1                1                           0.267925        0.807547                   168.679   168.679\n",
       "    7        0.400281                    0.33321            1.7029     2.44168            0.633803         0.908772                    0.169811        0.977358                   70.2897   144.168\n",
       "    8        0.5                         0.173686           0.151369   1.98491            0.056338         0.738764                    0.0150943       0.992453                   -84.8631  98.4906\n",
       "    9        0.609551                    0.107721           0.0688921  1.64055            0.025641         0.610599                    0.00754717      1                          -93.1108  64.0553\n",
       "    10       0.699438                    0.0658923          0          1.42972            0                0.532129                    0               1                          -100      42.9719\n",
       "    11       0.799157                    0.0438331          0          1.25132            0                0.465729                    0               1                          -100      25.1318\n",
       "    12       0.898876                    0.0221624          0          1.1125             0                0.414062                    0               1                          -100      11.25\n",
       "    13       1                           0                  0          1                  0                0.372191                    0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:33</td>\n",
       "<td> 0.011 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:33</td>\n",
       "<td> 0.204 sec</td>\n",
       "<td>1.0</td>\n",
       "<td>0.4291874</td>\n",
       "<td>5.9978498</td>\n",
       "<td>0.7669266</td>\n",
       "<td>2.4437017</td>\n",
       "<td>0.1839080</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:33</td>\n",
       "<td> 0.317 sec</td>\n",
       "<td>2.0</td>\n",
       "<td>0.4322233</td>\n",
       "<td>5.6811387</td>\n",
       "<td>0.7908389</td>\n",
       "<td>2.6141764</td>\n",
       "<td>0.1961259</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:33</td>\n",
       "<td> 0.367 sec</td>\n",
       "<td>3.0</td>\n",
       "<td>0.4428774</td>\n",
       "<td>5.7965230</td>\n",
       "<td>0.7724040</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.2011385</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:33</td>\n",
       "<td> 0.403 sec</td>\n",
       "<td>4.0</td>\n",
       "<td>0.4366525</td>\n",
       "<td>5.0403270</td>\n",
       "<td>0.7734675</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.2003367</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:34</td>\n",
       "<td> 1.616 sec</td>\n",
       "<td>46.0</td>\n",
       "<td>0.3706536</td>\n",
       "<td>0.4636126</td>\n",
       "<td>0.8561606</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1853933</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:34</td>\n",
       "<td> 1.652 sec</td>\n",
       "<td>47.0</td>\n",
       "<td>0.3717892</td>\n",
       "<td>0.4663608</td>\n",
       "<td>0.8544004</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1727528</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:34</td>\n",
       "<td> 1.688 sec</td>\n",
       "<td>48.0</td>\n",
       "<td>0.3715805</td>\n",
       "<td>0.4660573</td>\n",
       "<td>0.8550293</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1769663</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:34</td>\n",
       "<td> 1.758 sec</td>\n",
       "<td>49.0</td>\n",
       "<td>0.3708112</td>\n",
       "<td>0.4623615</td>\n",
       "<td>0.8564476</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1769663</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:34</td>\n",
       "<td> 1.802 sec</td>\n",
       "<td>50.0</td>\n",
       "<td>0.3700430</td>\n",
       "<td>0.4610866</td>\n",
       "<td>0.8571567</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1699438</td></tr></table></div>"
      ],
      "text/plain": [
       "     timestamp            duration    number_of_trees    training_rmse        training_logloss     training_auc        training_lift       training_classification_error\n",
       "---  -------------------  ----------  -----------------  -------------------  -------------------  ------------------  ------------------  -------------------------------\n",
       "     2017-04-06 14:32:33  0.011 sec   0.0                nan                  nan                  nan                 nan                 nan\n",
       "     2017-04-06 14:32:33  0.204 sec   1.0                0.4291873574382064   5.997849846746698    0.7669265756985055  2.4437017070979334  0.1839080459770115\n",
       "     2017-04-06 14:32:33  0.317 sec   2.0                0.4322233040379346   5.6811386657219565   0.7908389089037571  2.6141764405915353  0.19612590799031476\n",
       "     2017-04-06 14:32:33  0.367 sec   3.0                0.4428773880788786   5.796522989560667    0.7724039829302988  2.6867924528301885  0.20113851992409867\n",
       "     2017-04-06 14:32:33  0.403 sec   4.0                0.4366524615399937   5.0403270106286655   0.7734675093575504  2.6867924528301885  0.20033670033670034\n",
       "---  ---                  ---         ---                ---                  ---                  ---                 ---                 ---\n",
       "     2017-04-06 14:32:34  1.616 sec   46.0               0.37065363158888787  0.4636125625060299   0.8561605673040396  2.6867924528301885  0.1853932584269663\n",
       "     2017-04-06 14:32:34  1.652 sec   47.0               0.37178920237348906  0.4663607782594671   0.8544004052171711  2.6867924528301885  0.17275280898876405\n",
       "     2017-04-06 14:32:34  1.688 sec   48.0               0.3715805000920975   0.4660572597799091   0.8550293360347812  2.6867924528301885  0.17696629213483145\n",
       "     2017-04-06 14:32:34  1.758 sec   49.0               0.37081122145053086  0.46236148166886804  0.8564475961335528  2.6867924528301885  0.17696629213483145\n",
       "     2017-04-06 14:32:34  1.802 sec   50.0               0.37004295029511264  0.4610865952257547   0.8571567261829387  2.6867924528301885  0.1699438202247191"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>Sex</td>\n",
       "<td>1513.9517822</td>\n",
       "<td>1.0</td>\n",
       "<td>0.3113656</td></tr>\n",
       "<tr><td>Age</td>\n",
       "<td>1163.5933838</td>\n",
       "<td>0.7685802</td>\n",
       "<td>0.2393095</td></tr>\n",
       "<tr><td>Fare</td>\n",
       "<td>1132.6246338</td>\n",
       "<td>0.7481246</td>\n",
       "<td>0.2329403</td></tr>\n",
       "<tr><td>Pclass</td>\n",
       "<td>451.8060913</td>\n",
       "<td>0.2984283</td>\n",
       "<td>0.0929203</td></tr>\n",
       "<tr><td>SibSp</td>\n",
       "<td>263.0476379</td>\n",
       "<td>0.1737490</td>\n",
       "<td>0.0540995</td></tr>\n",
       "<tr><td>Embarked</td>\n",
       "<td>168.9029541</td>\n",
       "<td>0.1115643</td>\n",
       "<td>0.0347373</td></tr>\n",
       "<tr><td>Parch</td>\n",
       "<td>168.3693390</td>\n",
       "<td>0.1112118</td>\n",
       "<td>0.0346275</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance    percentage\n",
       "----------  ---------------------  -------------------  ------------\n",
       "Sex         1513.95                1                    0.311366\n",
       "Age         1163.59                0.76858              0.239309\n",
       "Fare        1132.62                0.748125             0.23294\n",
       "Pclass      451.806                0.298428             0.0929203\n",
       "SibSp       263.048                0.173749             0.0540995\n",
       "Embarked    168.903                0.111564             0.0347373\n",
       "Parch       168.369                0.111212             0.0346275"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the DRF model summary\n",
    "drf_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomial: drf\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.1274604300255528\n",
      "RMSE: 0.3570160080802439\n",
      "LogLoss: 0.41184768288178214\n",
      "Mean Per-Class Error: 0.1690858161446397\n",
      "AUC: 0.8883371530430353\n",
      "Gini: 0.7766743060860706\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5416861137747764: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>94.0</td>\n",
       "<td>8.0</td>\n",
       "<td>0.0784</td>\n",
       "<td> (8.0/102.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>20.0</td>\n",
       "<td>57.0</td>\n",
       "<td>0.2597</td>\n",
       "<td> (20.0/77.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>114.0</td>\n",
       "<td>65.0</td>\n",
       "<td>0.1564</td>\n",
       "<td> (28.0/179.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  ------------\n",
       "0      94   8    0.0784   (8.0/102.0)\n",
       "1      20   57   0.2597   (20.0/77.0)\n",
       "Total  114  65   0.1564   (28.0/179.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.5416861</td>\n",
       "<td>0.8028169</td>\n",
       "<td>56.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1513190</td>\n",
       "<td>0.8488372</td>\n",
       "<td>112.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5672688</td>\n",
       "<td>0.8510638</td>\n",
       "<td>54.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5672688</td>\n",
       "<td>0.8435754</td>\n",
       "<td>54.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0101172</td>\n",
       "<td>1.0</td>\n",
       "<td>153.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5672688</td>\n",
       "<td>0.6828067</td>\n",
       "<td>54.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.3550097</td>\n",
       "<td>0.8051948</td>\n",
       "<td>70.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5416861</td>\n",
       "<td>0.8309142</td>\n",
       "<td>56.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.541686     0.802817  56\n",
       "max f2                       0.151319     0.848837  112\n",
       "max f0point5                 0.567269     0.851064  54\n",
       "max accuracy                 0.567269     0.843575  54\n",
       "max precision                1            1         0\n",
       "max recall                   0.0101172    1         153\n",
       "max specificity              1            1         0\n",
       "max absolute_mcc             0.567269     0.682807  54\n",
       "max min_per_class_accuracy   0.35501      0.805195  70\n",
       "max mean_per_class_accuracy  0.541686     0.830914  56"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 43.02 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0335196</td>\n",
       "<td>0.9995745</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.0779221</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0335196</td>\n",
       "<td>0.9927745</td>\n",
       "<td>0.0</td>\n",
       "<td>2.3246753</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0779221</td>\n",
       "<td>-100.0</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0446927</td>\n",
       "<td>0.9781327</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.1038961</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0502793</td>\n",
       "<td>0.9691349</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.1168831</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.1005587</td>\n",
       "<td>0.9319590</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1168831</td>\n",
       "<td>0.2337662</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1564246</td>\n",
       "<td>0.8745745</td>\n",
       "<td>2.0922078</td>\n",
       "<td>2.2416512</td>\n",
       "<td>0.9</td>\n",
       "<td>0.9642857</td>\n",
       "<td>0.1168831</td>\n",
       "<td>0.3506494</td>\n",
       "<td>109.2207792</td>\n",
       "<td>124.1651206</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.2067039</td>\n",
       "<td>0.8625339</td>\n",
       "<td>1.8080808</td>\n",
       "<td>2.1361881</td>\n",
       "<td>0.7777778</td>\n",
       "<td>0.9189189</td>\n",
       "<td>0.0909091</td>\n",
       "<td>0.4415584</td>\n",
       "<td>80.8080808</td>\n",
       "<td>113.6188136</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.3016760</td>\n",
       "<td>0.6292381</td>\n",
       "<td>2.0511841</td>\n",
       "<td>2.1094276</td>\n",
       "<td>0.8823529</td>\n",
       "<td>0.9074074</td>\n",
       "<td>0.1948052</td>\n",
       "<td>0.6363636</td>\n",
       "<td>105.1184110</td>\n",
       "<td>110.9427609</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.4022346</td>\n",
       "<td>0.4531885</td>\n",
       "<td>1.2914863</td>\n",
       "<td>1.9049423</td>\n",
       "<td>0.5555556</td>\n",
       "<td>0.8194444</td>\n",
       "<td>0.1298701</td>\n",
       "<td>0.7662338</td>\n",
       "<td>29.1486291</td>\n",
       "<td>90.4942280</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.5027933</td>\n",
       "<td>0.2850131</td>\n",
       "<td>0.7748918</td>\n",
       "<td>1.6789322</td>\n",
       "<td>0.3333333</td>\n",
       "<td>0.7222222</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.8441558</td>\n",
       "<td>-22.5108225</td>\n",
       "<td>67.8932179</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5977654</td>\n",
       "<td>0.2027503</td>\n",
       "<td>0.4102368</td>\n",
       "<td>1.4773638</td>\n",
       "<td>0.1764706</td>\n",
       "<td>0.6355140</td>\n",
       "<td>0.0389610</td>\n",
       "<td>0.8831169</td>\n",
       "<td>-58.9763178</td>\n",
       "<td>47.7363758</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.7262570</td>\n",
       "<td>0.1353284</td>\n",
       "<td>0.5053642</td>\n",
       "<td>1.3053946</td>\n",
       "<td>0.2173913</td>\n",
       "<td>0.5615385</td>\n",
       "<td>0.0649351</td>\n",
       "<td>0.9480519</td>\n",
       "<td>-49.4635799</td>\n",
       "<td>30.5394605</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7988827</td>\n",
       "<td>0.0743745</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1867224</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5104895</td>\n",
       "<td>0.0</td>\n",
       "<td>0.9480519</td>\n",
       "<td>-100.0</td>\n",
       "<td>18.6722369</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8994413</td>\n",
       "<td>0.0400742</td>\n",
       "<td>0.3874459</td>\n",
       "<td>1.0973623</td>\n",
       "<td>0.1666667</td>\n",
       "<td>0.4720497</td>\n",
       "<td>0.0389610</td>\n",
       "<td>0.9870130</td>\n",
       "<td>-61.2554113</td>\n",
       "<td>9.7362265</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0080119</td>\n",
       "<td>0.1291486</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0555556</td>\n",
       "<td>0.4301676</td>\n",
       "<td>0.0129870</td>\n",
       "<td>1.0</td>\n",
       "<td>-87.0851371</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0335196                   0.999574           2.32468   2.32468            1                1                           0.0779221       0.0779221                  132.468   132.468\n",
       "    2        0.0335196                   0.992774           0         2.32468            0                1                           0               0.0779221                  -100      132.468\n",
       "    3        0.0446927                   0.978133           2.32468   2.32468            1                1                           0.025974        0.103896                   132.468   132.468\n",
       "    4        0.0502793                   0.969135           2.32468   2.32468            1                1                           0.012987        0.116883                   132.468   132.468\n",
       "    5        0.100559                    0.931959           2.32468   2.32468            1                1                           0.116883        0.233766                   132.468   132.468\n",
       "    6        0.156425                    0.874574           2.09221   2.24165            0.9              0.964286                    0.116883        0.350649                   109.221   124.165\n",
       "    7        0.206704                    0.862534           1.80808   2.13619            0.777778         0.918919                    0.0909091       0.441558                   80.8081   113.619\n",
       "    8        0.301676                    0.629238           2.05118   2.10943            0.882353         0.907407                    0.194805        0.636364                   105.118   110.943\n",
       "    9        0.402235                    0.453189           1.29149   1.90494            0.555556         0.819444                    0.12987         0.766234                   29.1486   90.4942\n",
       "    10       0.502793                    0.285013           0.774892  1.67893            0.333333         0.722222                    0.0779221       0.844156                   -22.5108  67.8932\n",
       "    11       0.597765                    0.20275            0.410237  1.47736            0.176471         0.635514                    0.038961        0.883117                   -58.9763  47.7364\n",
       "    12       0.726257                    0.135328           0.505364  1.30539            0.217391         0.561538                    0.0649351       0.948052                   -49.4636  30.5395\n",
       "    13       0.798883                    0.0743745          0         1.18672            0                0.51049                     0               0.948052                   -100      18.6722\n",
       "    14       0.899441                    0.0400742          0.387446  1.09736            0.166667         0.47205                     0.038961        0.987013                   -61.2554  9.73623\n",
       "    15       1                           0.0080119          0.129149  1                  0.0555556        0.430168                    0.012987        1                          -87.0851  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model performance on test dataset\n",
    "drf_default.model_performance(titanic_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "\n",
    "## Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Build a Gradient Boosting Machines (GBM) model with default settings\n",
    "\n",
    "# Import the function for GBM\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "# Set up GBM for regression\n",
    "# Add a seed for reproducibility\n",
    "gbm_default = H2OGradientBoostingEstimator(model_id = 'gbm_default', seed = 1234)\n",
    "\n",
    "# Use .train() to build the model\n",
    "gbm_default.train(x = features, \n",
    "                  y = 'Survived', \n",
    "                  training_frame = titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGradientBoostingEstimator :  Gradient Boosting Machine\n",
      "Model Key:  gbm_default\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.07758897316685097\n",
      "RMSE: 0.2785479728284716\n",
      "LogLoss: 0.2713297688833869\n",
      "Mean Per-Class Error: 0.1047697437845595\n",
      "AUC: 0.9547465282174665\n",
      "Gini: 0.9094930564349331\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.414932842273446: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>417.0</td>\n",
       "<td>30.0</td>\n",
       "<td>0.0671</td>\n",
       "<td> (30.0/447.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>38.0</td>\n",
       "<td>227.0</td>\n",
       "<td>0.1434</td>\n",
       "<td> (38.0/265.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>455.0</td>\n",
       "<td>257.0</td>\n",
       "<td>0.0955</td>\n",
       "<td> (68.0/712.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  ------------\n",
       "0      417  30   0.0671   (30.0/447.0)\n",
       "1      38   227  0.1434   (38.0/265.0)\n",
       "Total  455  257  0.0955   (68.0/712.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4149328</td>\n",
       "<td>0.8697318</td>\n",
       "<td>182.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.3208260</td>\n",
       "<td>0.8792846</td>\n",
       "<td>202.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6537940</td>\n",
       "<td>0.9107981</td>\n",
       "<td>133.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4816206</td>\n",
       "<td>0.9073034</td>\n",
       "<td>167.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9901771</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0878633</td>\n",
       "<td>1.0</td>\n",
       "<td>332.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9901771</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4816206</td>\n",
       "<td>0.8000835</td>\n",
       "<td>167.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.3208260</td>\n",
       "<td>0.8905660</td>\n",
       "<td>202.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.3904755</td>\n",
       "<td>0.8952303</td>\n",
       "<td>192.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.414933     0.869732  182\n",
       "max f2                       0.320826     0.879285  202\n",
       "max f0point5                 0.653794     0.910798  133\n",
       "max accuracy                 0.481621     0.907303  167\n",
       "max precision                0.990177     1         0\n",
       "max recall                   0.0878633    1         332\n",
       "max specificity              0.990177     1         0\n",
       "max absolute_mcc             0.481621     0.800083  167\n",
       "max min_per_class_accuracy   0.320826     0.890566  202\n",
       "max mean_per_class_accuracy  0.390476     0.89523   192"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 37.22 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0112360</td>\n",
       "<td>0.9866256</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0301887</td>\n",
       "<td>0.0301887</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0210674</td>\n",
       "<td>0.9827783</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.0566038</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0308989</td>\n",
       "<td>0.9801608</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.0830189</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0407303</td>\n",
       "<td>0.9790456</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.1094340</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0505618</td>\n",
       "<td>0.9752566</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.1358491</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1011236</td>\n",
       "<td>0.9606552</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1358491</td>\n",
       "<td>0.2716981</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1502809</td>\n",
       "<td>0.9243655</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1320755</td>\n",
       "<td>0.4037736</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2008427</td>\n",
       "<td>0.8073383</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1358491</td>\n",
       "<td>0.5396226</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3005618</td>\n",
       "<td>0.5918481</td>\n",
       "<td>2.1948445</td>\n",
       "<td>2.5235761</td>\n",
       "<td>0.8169014</td>\n",
       "<td>0.9392523</td>\n",
       "<td>0.2188679</td>\n",
       "<td>0.7584906</td>\n",
       "<td>119.4844539</td>\n",
       "<td>152.3576089</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4002809</td>\n",
       "<td>0.3038840</td>\n",
       "<td>1.3244752</td>\n",
       "<td>2.2248527</td>\n",
       "<td>0.4929577</td>\n",
       "<td>0.8280702</td>\n",
       "<td>0.1320755</td>\n",
       "<td>0.8905660</td>\n",
       "<td>32.4475153</td>\n",
       "<td>122.4852698</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1634237</td>\n",
       "<td>0.4162636</td>\n",
       "<td>1.8641509</td>\n",
       "<td>0.1549296</td>\n",
       "<td>0.6938202</td>\n",
       "<td>0.0415094</td>\n",
       "<td>0.9320755</td>\n",
       "<td>-58.3736381</td>\n",
       "<td>86.4150943</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6025281</td>\n",
       "<td>0.1231070</td>\n",
       "<td>0.2944430</td>\n",
       "<td>1.5970445</td>\n",
       "<td>0.1095890</td>\n",
       "<td>0.5944056</td>\n",
       "<td>0.0301887</td>\n",
       "<td>0.9622642</td>\n",
       "<td>-70.5556991</td>\n",
       "<td>59.7044465</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6994382</td>\n",
       "<td>0.0977094</td>\n",
       "<td>0.2725731</td>\n",
       "<td>1.4135334</td>\n",
       "<td>0.1014493</td>\n",
       "<td>0.5261044</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.9886792</td>\n",
       "<td>-72.7426853</td>\n",
       "<td>41.3533379</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7991573</td>\n",
       "<td>0.0871370</td>\n",
       "<td>0.1135264</td>\n",
       "<td>1.2513181</td>\n",
       "<td>0.0422535</td>\n",
       "<td>0.4657293</td>\n",
       "<td>0.0113208</td>\n",
       "<td>1.0</td>\n",
       "<td>-88.6473558</td>\n",
       "<td>25.1318102</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8988764</td>\n",
       "<td>0.0657414</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1125</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4140625</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.2500000</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0198764</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.3721910</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.011236                    0.986626           2.68679   2.68679            1                1                           0.0301887       0.0301887                  168.679   168.679\n",
       "    2        0.0210674                   0.982778           2.68679   2.68679            1                1                           0.0264151       0.0566038                  168.679   168.679\n",
       "    3        0.0308989                   0.980161           2.68679   2.68679            1                1                           0.0264151       0.0830189                  168.679   168.679\n",
       "    4        0.0407303                   0.979046           2.68679   2.68679            1                1                           0.0264151       0.109434                   168.679   168.679\n",
       "    5        0.0505618                   0.975257           2.68679   2.68679            1                1                           0.0264151       0.135849                   168.679   168.679\n",
       "    6        0.101124                    0.960655           2.68679   2.68679            1                1                           0.135849        0.271698                   168.679   168.679\n",
       "    7        0.150281                    0.924365           2.68679   2.68679            1                1                           0.132075        0.403774                   168.679   168.679\n",
       "    8        0.200843                    0.807338           2.68679   2.68679            1                1                           0.135849        0.539623                   168.679   168.679\n",
       "    9        0.300562                    0.591848           2.19484   2.52358            0.816901         0.939252                    0.218868        0.758491                   119.484   152.358\n",
       "    10       0.400281                    0.303884           1.32448   2.22485            0.492958         0.82807                     0.132075        0.890566                   32.4475   122.485\n",
       "    11       0.5                         0.163424           0.416264  1.86415            0.15493          0.69382                     0.0415094       0.932075                   -58.3736  86.4151\n",
       "    12       0.602528                    0.123107           0.294443  1.59704            0.109589         0.594406                    0.0301887       0.962264                   -70.5557  59.7044\n",
       "    13       0.699438                    0.0977094          0.272573  1.41353            0.101449         0.526104                    0.0264151       0.988679                   -72.7427  41.3533\n",
       "    14       0.799157                    0.087137           0.113526  1.25132            0.0422535        0.465729                    0.0113208       1                          -88.6474  25.1318\n",
       "    15       0.898876                    0.0657414          0         1.1125             0                0.414062                    0               1                          -100      11.25\n",
       "    16       1                           0.0198764          0         1                  0                0.372191                    0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:35</td>\n",
       "<td> 0.004 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4833889</td>\n",
       "<td>0.6601115</td>\n",
       "<td>0.5</td>\n",
       "<td>1.0</td>\n",
       "<td>0.6278090</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:35</td>\n",
       "<td> 0.064 sec</td>\n",
       "<td>1.0</td>\n",
       "<td>0.4598999</td>\n",
       "<td>0.6132126</td>\n",
       "<td>0.8876493</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1671348</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:35</td>\n",
       "<td> 0.088 sec</td>\n",
       "<td>2.0</td>\n",
       "<td>0.4400875</td>\n",
       "<td>0.5757196</td>\n",
       "<td>0.8964248</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1755618</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:35</td>\n",
       "<td> 0.103 sec</td>\n",
       "<td>3.0</td>\n",
       "<td>0.4235059</td>\n",
       "<td>0.5452385</td>\n",
       "<td>0.8968469</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1685393</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:35</td>\n",
       "<td> 0.124 sec</td>\n",
       "<td>4.0</td>\n",
       "<td>0.4093690</td>\n",
       "<td>0.5196299</td>\n",
       "<td>0.9012874</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1601124</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:36</td>\n",
       "<td> 1.055 sec</td>\n",
       "<td>46.0</td>\n",
       "<td>0.2832313</td>\n",
       "<td>0.2788065</td>\n",
       "<td>0.9512642</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1011236</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:36</td>\n",
       "<td> 1.078 sec</td>\n",
       "<td>47.0</td>\n",
       "<td>0.2818163</td>\n",
       "<td>0.2767664</td>\n",
       "<td>0.9518551</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1025281</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:36</td>\n",
       "<td> 1.106 sec</td>\n",
       "<td>48.0</td>\n",
       "<td>0.2812117</td>\n",
       "<td>0.2757954</td>\n",
       "<td>0.9523954</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1011236</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:36</td>\n",
       "<td> 1.130 sec</td>\n",
       "<td>49.0</td>\n",
       "<td>0.2798017</td>\n",
       "<td>0.2734512</td>\n",
       "<td>0.9533620</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.0997191</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:36</td>\n",
       "<td> 1.156 sec</td>\n",
       "<td>50.0</td>\n",
       "<td>0.2785480</td>\n",
       "<td>0.2713298</td>\n",
       "<td>0.9547465</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.0955056</td></tr></table></div>"
      ],
      "text/plain": [
       "     timestamp            duration    number_of_trees    training_rmse        training_logloss    training_auc        training_lift       training_classification_error\n",
       "---  -------------------  ----------  -----------------  -------------------  ------------------  ------------------  ------------------  -------------------------------\n",
       "     2017-04-06 14:32:35  0.004 sec   0.0                0.4833889349076107   0.6601114848970648  0.5                 1.0                 0.6278089887640449\n",
       "     2017-04-06 14:32:35  0.064 sec   1.0                0.45989986689221923  0.6132125941626966  0.8876493183065299  2.6867924528301885  0.16713483146067415\n",
       "     2017-04-06 14:32:35  0.088 sec   2.0                0.4400875187902207   0.5757195983670309  0.8964248026676798  2.6867924528301885  0.175561797752809\n",
       "     2017-04-06 14:32:35  0.103 sec   3.0                0.4235058566314087   0.5452385076204875  0.8968469038875522  2.6867924528301885  0.16853932584269662\n",
       "     2017-04-06 14:32:35  0.124 sec   4.0                0.40936898295634916  0.5196299221482236  0.9012874087206112  2.6867924528301885  0.1601123595505618\n",
       "---  ---                  ---         ---                ---                  ---                 ---                 ---                 ---\n",
       "     2017-04-06 14:32:36  1.055 sec   46.0               0.2832313313042898   0.2788065370744706  0.9512641931535182  2.6867924528301885  0.10112359550561797\n",
       "     2017-04-06 14:32:36  1.078 sec   47.0               0.28181627994627284  0.2767663993022549  0.9518551348613398  2.6867924528301885  0.10252808988764045\n",
       "     2017-04-06 14:32:36  1.106 sec   48.0               0.2812116794952194   0.2757954350818175  0.9523954244227765  2.6867924528301885  0.10112359550561797\n",
       "     2017-04-06 14:32:36  1.130 sec   49.0               0.2798016840952948   0.2734512350771905  0.9533620362162846  2.6867924528301885  0.0997191011235955\n",
       "     2017-04-06 14:32:36  1.156 sec   50.0               0.2785479728284716   0.2713297688833869  0.9547465282174665  2.6867924528301885  0.09550561797752809"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>Sex</td>\n",
       "<td>269.9861755</td>\n",
       "<td>1.0</td>\n",
       "<td>0.4711521</td></tr>\n",
       "<tr><td>Age</td>\n",
       "<td>103.5973969</td>\n",
       "<td>0.3837137</td>\n",
       "<td>0.1807875</td></tr>\n",
       "<tr><td>Pclass</td>\n",
       "<td>80.2009659</td>\n",
       "<td>0.2970558</td>\n",
       "<td>0.1399585</td></tr>\n",
       "<tr><td>Fare</td>\n",
       "<td>77.6247101</td>\n",
       "<td>0.2875136</td>\n",
       "<td>0.1354627</td></tr>\n",
       "<tr><td>SibSp</td>\n",
       "<td>29.2148132</td>\n",
       "<td>0.1082086</td>\n",
       "<td>0.0509827</td></tr>\n",
       "<tr><td>Embarked</td>\n",
       "<td>9.1480532</td>\n",
       "<td>0.0338834</td>\n",
       "<td>0.0159642</td></tr>\n",
       "<tr><td>Parch</td>\n",
       "<td>3.2619276</td>\n",
       "<td>0.0120818</td>\n",
       "<td>0.0056924</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance    percentage\n",
       "----------  ---------------------  -------------------  ------------\n",
       "Sex         269.986                1                    0.471152\n",
       "Age         103.597                0.383714             0.180788\n",
       "Pclass      80.201                 0.297056             0.139958\n",
       "Fare        77.6247                0.287514             0.135463\n",
       "SibSp       29.2148                0.108209             0.0509827\n",
       "Embarked    9.14805                0.0338834            0.0159642\n",
       "Parch       3.26193                0.0120818            0.00569238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the GBM model summary\n",
    "gbm_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.12599930735750908\n",
      "RMSE: 0.3549638113350558\n",
      "LogLoss: 0.40967742175755323\n",
      "Mean Per-Class Error: 0.16603004838298951\n",
      "AUC: 0.8834351922587216\n",
      "Gini: 0.7668703845174432\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.40893590243834643: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>88.0</td>\n",
       "<td>14.0</td>\n",
       "<td>0.1373</td>\n",
       "<td> (14.0/102.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>15.0</td>\n",
       "<td>62.0</td>\n",
       "<td>0.1948</td>\n",
       "<td> (15.0/77.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>103.0</td>\n",
       "<td>76.0</td>\n",
       "<td>0.162</td>\n",
       "<td> (29.0/179.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  ------------\n",
       "0      88   14   0.1373   (14.0/102.0)\n",
       "1      15   62   0.1948   (15.0/77.0)\n",
       "Total  103  76   0.162    (29.0/179.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4089359</td>\n",
       "<td>0.8104575</td>\n",
       "<td>68.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1739562</td>\n",
       "<td>0.8658537</td>\n",
       "<td>90.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6215871</td>\n",
       "<td>0.8626198</td>\n",
       "<td>51.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6215871</td>\n",
       "<td>0.8435754</td>\n",
       "<td>51.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9874823</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0607561</td>\n",
       "<td>1.0</td>\n",
       "<td>142.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9874823</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.6215871</td>\n",
       "<td>0.6870088</td>\n",
       "<td>51.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2946931</td>\n",
       "<td>0.8181818</td>\n",
       "<td>73.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4089359</td>\n",
       "<td>0.8339700</td>\n",
       "<td>68.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.408936     0.810458  68\n",
       "max f2                       0.173956     0.865854  90\n",
       "max f0point5                 0.621587     0.86262   51\n",
       "max accuracy                 0.621587     0.843575  51\n",
       "max precision                0.987482     1         0\n",
       "max recall                   0.0607561    1         142\n",
       "max specificity              0.987482     1         0\n",
       "max absolute_mcc             0.621587     0.687009  51\n",
       "max min_per_class_accuracy   0.294693     0.818182  73\n",
       "max mean_per_class_accuracy  0.408936     0.83397   68"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 43.02 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0111732</td>\n",
       "<td>0.9855215</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0259740</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0223464</td>\n",
       "<td>0.9815861</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0519481</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0335196</td>\n",
       "<td>0.9785022</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0779221</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0446927</td>\n",
       "<td>0.9766887</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.1038961</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0502793</td>\n",
       "<td>0.9709074</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.1168831</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1005587</td>\n",
       "<td>0.9594569</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.1955267</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.2207792</td>\n",
       "<td>106.6378066</td>\n",
       "<td>119.5526696</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1508380</td>\n",
       "<td>0.9260351</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.1524772</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9259259</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.3246753</td>\n",
       "<td>106.6378066</td>\n",
       "<td>115.2477152</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2011173</td>\n",
       "<td>0.8804140</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.1309524</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9166667</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.4285714</td>\n",
       "<td>106.6378066</td>\n",
       "<td>113.0952381</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3016760</td>\n",
       "<td>0.6805693</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.1094276</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9074074</td>\n",
       "<td>0.2077922</td>\n",
       "<td>0.6363636</td>\n",
       "<td>106.6378066</td>\n",
       "<td>110.9427609</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4022346</td>\n",
       "<td>0.4574617</td>\n",
       "<td>1.2914863</td>\n",
       "<td>1.9049423</td>\n",
       "<td>0.5555556</td>\n",
       "<td>0.8194444</td>\n",
       "<td>0.1298701</td>\n",
       "<td>0.7662338</td>\n",
       "<td>29.1486291</td>\n",
       "<td>90.4942280</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5027933</td>\n",
       "<td>0.2239804</td>\n",
       "<td>0.7748918</td>\n",
       "<td>1.6789322</td>\n",
       "<td>0.3333333</td>\n",
       "<td>0.7222222</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.8441558</td>\n",
       "<td>-22.5108225</td>\n",
       "<td>67.8932179</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6089385</td>\n",
       "<td>0.1669759</td>\n",
       "<td>0.7341080</td>\n",
       "<td>1.5142381</td>\n",
       "<td>0.3157895</td>\n",
       "<td>0.6513761</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.9220779</td>\n",
       "<td>-26.5892003</td>\n",
       "<td>51.4238056</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6983240</td>\n",
       "<td>0.1096594</td>\n",
       "<td>0.1452922</td>\n",
       "<td>1.3390130</td>\n",
       "<td>0.0625</td>\n",
       "<td>0.576</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.9350649</td>\n",
       "<td>-85.4707792</td>\n",
       "<td>33.9012987</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7988827</td>\n",
       "<td>0.0897154</td>\n",
       "<td>0.1291486</td>\n",
       "<td>1.1867224</td>\n",
       "<td>0.0555556</td>\n",
       "<td>0.5104895</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.9480519</td>\n",
       "<td>-87.0851371</td>\n",
       "<td>18.6722369</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8994413</td>\n",
       "<td>0.0758639</td>\n",
       "<td>0.1291486</td>\n",
       "<td>1.0684843</td>\n",
       "<td>0.0555556</td>\n",
       "<td>0.4596273</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.9610390</td>\n",
       "<td>-87.0851371</td>\n",
       "<td>6.8484311</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0372517</td>\n",
       "<td>0.3874459</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1666667</td>\n",
       "<td>0.4301676</td>\n",
       "<td>0.0389610</td>\n",
       "<td>1.0</td>\n",
       "<td>-61.2554113</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0111732                   0.985522           2.32468   2.32468            1                1                           0.025974        0.025974                   132.468   132.468\n",
       "    2        0.0223464                   0.981586           2.32468   2.32468            1                1                           0.025974        0.0519481                  132.468   132.468\n",
       "    3        0.0335196                   0.978502           2.32468   2.32468            1                1                           0.025974        0.0779221                  132.468   132.468\n",
       "    4        0.0446927                   0.976689           2.32468   2.32468            1                1                           0.025974        0.103896                   132.468   132.468\n",
       "    5        0.0502793                   0.970907           2.32468   2.32468            1                1                           0.012987        0.116883                   132.468   132.468\n",
       "    6        0.100559                    0.959457           2.06638   2.19553            0.888889         0.944444                    0.103896        0.220779                   106.638   119.553\n",
       "    7        0.150838                    0.926035           2.06638   2.15248            0.888889         0.925926                    0.103896        0.324675                   106.638   115.248\n",
       "    8        0.201117                    0.880414           2.06638   2.13095            0.888889         0.916667                    0.103896        0.428571                   106.638   113.095\n",
       "    9        0.301676                    0.680569           2.06638   2.10943            0.888889         0.907407                    0.207792        0.636364                   106.638   110.943\n",
       "    10       0.402235                    0.457462           1.29149   1.90494            0.555556         0.819444                    0.12987         0.766234                   29.1486   90.4942\n",
       "    11       0.502793                    0.22398            0.774892  1.67893            0.333333         0.722222                    0.0779221       0.844156                   -22.5108  67.8932\n",
       "    12       0.608939                    0.166976           0.734108  1.51424            0.315789         0.651376                    0.0779221       0.922078                   -26.5892  51.4238\n",
       "    13       0.698324                    0.109659           0.145292  1.33901            0.0625           0.576                       0.012987        0.935065                   -85.4708  33.9013\n",
       "    14       0.798883                    0.0897154          0.129149  1.18672            0.0555556        0.51049                     0.012987        0.948052                   -87.0851  18.6722\n",
       "    15       0.899441                    0.0758639          0.129149  1.06848            0.0555556        0.459627                    0.012987        0.961039                   -87.0851  6.84843\n",
       "    16       1                           0.0372517          0.387446  1                  0.166667         0.430168                    0.038961        1                          -61.2554  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model performance on test dataset\n",
    "gbm_default.model_performance(titanic_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## H2O Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning Model Build progress: |██████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Build a Deep Learning (Deep Neural Networks, DNN) model with default settings\n",
    "\n",
    "# Import the function for DNN\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "\n",
    "# Set up DNN for regression\n",
    "dnn_default = H2ODeepLearningEstimator(model_id = 'dnn_default')\n",
    "\n",
    "# (not run) Change 'reproducible' to True if you want to reproduce the results\n",
    "# The model will be built using a single thread (could be very slow)\n",
    "# dnn_default = H2ODeepLearningEstimator(model_id = 'dnn_default', reproducible = True)\n",
    "\n",
    "# Use .train() to build the model\n",
    "dnn_default.train(x = features, \n",
    "                  y = 'Survived', \n",
    "                  training_frame = titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2ODeepLearningEstimator :  Deep Learning\n",
      "Model Key:  dnn_default\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.12594366835501403\n",
      "RMSE: 0.3548854298995861\n",
      "LogLoss: 0.4226623252768216\n",
      "Mean Per-Class Error: 0.1847832510235955\n",
      "AUC: 0.8828880165463678\n",
      "Gini: 0.7657760330927357\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4420577081657904: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>410.0</td>\n",
       "<td>37.0</td>\n",
       "<td>0.0828</td>\n",
       "<td> (37.0/447.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>76.0</td>\n",
       "<td>189.0</td>\n",
       "<td>0.2868</td>\n",
       "<td> (76.0/265.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>486.0</td>\n",
       "<td>226.0</td>\n",
       "<td>0.1587</td>\n",
       "<td> (113.0/712.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  -------------\n",
       "0      410  37   0.0828   (37.0/447.0)\n",
       "1      76   189  0.2868   (76.0/265.0)\n",
       "Total  486  226  0.1587   (113.0/712.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4420577</td>\n",
       "<td>0.7698574</td>\n",
       "<td>162.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0841420</td>\n",
       "<td>0.8151023</td>\n",
       "<td>268.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5719712</td>\n",
       "<td>0.8341277</td>\n",
       "<td>135.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5719712</td>\n",
       "<td>0.8441011</td>\n",
       "<td>135.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9998282</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0039936</td>\n",
       "<td>1.0</td>\n",
       "<td>393.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9998282</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5719712</td>\n",
       "<td>0.6638437</td>\n",
       "<td>135.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1872703</td>\n",
       "<td>0.8008949</td>\n",
       "<td>229.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4420577</td>\n",
       "<td>0.8152167</td>\n",
       "<td>162.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.442058     0.769857  162\n",
       "max f2                       0.084142     0.815102  268\n",
       "max f0point5                 0.571971     0.834128  135\n",
       "max accuracy                 0.571971     0.844101  135\n",
       "max precision                0.999828     1         0\n",
       "max recall                   0.00399356   1         393\n",
       "max specificity              0.999828     1         0\n",
       "max absolute_mcc             0.571971     0.663844  135\n",
       "max min_per_class_accuracy   0.18727      0.800895  229\n",
       "max mean_per_class_accuracy  0.442058     0.815217  162"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 37.22 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0112360</td>\n",
       "<td>0.9965569</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0301887</td>\n",
       "<td>0.0301887</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0210674</td>\n",
       "<td>0.9962397</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.0566038</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0308989</td>\n",
       "<td>0.9957517</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.0830189</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0407303</td>\n",
       "<td>0.9946659</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.1094340</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0505618</td>\n",
       "<td>0.9937823</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.1358491</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1011236</td>\n",
       "<td>0.9704896</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1358491</td>\n",
       "<td>0.2716981</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1502809</td>\n",
       "<td>0.9164898</td>\n",
       "<td>2.3029650</td>\n",
       "<td>2.5612414</td>\n",
       "<td>0.8571429</td>\n",
       "<td>0.9532710</td>\n",
       "<td>0.1132075</td>\n",
       "<td>0.3849057</td>\n",
       "<td>130.2964960</td>\n",
       "<td>156.1241404</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2008427</td>\n",
       "<td>0.7910605</td>\n",
       "<td>2.4628931</td>\n",
       "<td>2.5364824</td>\n",
       "<td>0.9166667</td>\n",
       "<td>0.9440559</td>\n",
       "<td>0.1245283</td>\n",
       "<td>0.5094340</td>\n",
       "<td>146.2893082</td>\n",
       "<td>153.6482386</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3005618</td>\n",
       "<td>0.4898387</td>\n",
       "<td>1.8164231</td>\n",
       "<td>2.2975842</td>\n",
       "<td>0.6760563</td>\n",
       "<td>0.8551402</td>\n",
       "<td>0.1811321</td>\n",
       "<td>0.6905660</td>\n",
       "<td>81.6423067</td>\n",
       "<td>129.7584200</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4002809</td>\n",
       "<td>0.2328382</td>\n",
       "<td>0.8703694</td>\n",
       "<td>1.9420324</td>\n",
       "<td>0.3239437</td>\n",
       "<td>0.7228070</td>\n",
       "<td>0.0867925</td>\n",
       "<td>0.7773585</td>\n",
       "<td>-12.9630614</td>\n",
       "<td>94.2032440</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0840864</td>\n",
       "<td>0.9460537</td>\n",
       "<td>1.7433962</td>\n",
       "<td>0.3521127</td>\n",
       "<td>0.6488764</td>\n",
       "<td>0.0943396</td>\n",
       "<td>0.8716981</td>\n",
       "<td>-5.3946319</td>\n",
       "<td>74.3396226</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5997191</td>\n",
       "<td>0.0525291</td>\n",
       "<td>0.2648950</td>\n",
       "<td>1.4975564</td>\n",
       "<td>0.0985915</td>\n",
       "<td>0.5573770</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.8981132</td>\n",
       "<td>-73.5104969</td>\n",
       "<td>49.7556449</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6994382</td>\n",
       "<td>0.0374313</td>\n",
       "<td>0.3405793</td>\n",
       "<td>1.3326059</td>\n",
       "<td>0.1267606</td>\n",
       "<td>0.4959839</td>\n",
       "<td>0.0339623</td>\n",
       "<td>0.9320755</td>\n",
       "<td>-65.9420675</td>\n",
       "<td>33.2605895</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7991573</td>\n",
       "<td>0.0275965</td>\n",
       "<td>0.3784215</td>\n",
       "<td>1.2135425</td>\n",
       "<td>0.1408451</td>\n",
       "<td>0.4516696</td>\n",
       "<td>0.0377358</td>\n",
       "<td>0.9698113</td>\n",
       "<td>-62.1578528</td>\n",
       "<td>21.3542461</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8988764</td>\n",
       "<td>0.0159173</td>\n",
       "<td>0.1513686</td>\n",
       "<td>1.0957075</td>\n",
       "<td>0.0563380</td>\n",
       "<td>0.4078125</td>\n",
       "<td>0.0150943</td>\n",
       "<td>0.9849057</td>\n",
       "<td>-84.8631411</td>\n",
       "<td>9.5707547</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0001101</td>\n",
       "<td>0.1492662</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0555556</td>\n",
       "<td>0.3721910</td>\n",
       "<td>0.0150943</td>\n",
       "<td>1.0</td>\n",
       "<td>-85.0733753</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.011236                    0.996557           2.68679   2.68679            1                1                           0.0301887       0.0301887                  168.679   168.679\n",
       "    2        0.0210674                   0.99624            2.68679   2.68679            1                1                           0.0264151       0.0566038                  168.679   168.679\n",
       "    3        0.0308989                   0.995752           2.68679   2.68679            1                1                           0.0264151       0.0830189                  168.679   168.679\n",
       "    4        0.0407303                   0.994666           2.68679   2.68679            1                1                           0.0264151       0.109434                   168.679   168.679\n",
       "    5        0.0505618                   0.993782           2.68679   2.68679            1                1                           0.0264151       0.135849                   168.679   168.679\n",
       "    6        0.101124                    0.97049            2.68679   2.68679            1                1                           0.135849        0.271698                   168.679   168.679\n",
       "    7        0.150281                    0.91649            2.30296   2.56124            0.857143         0.953271                    0.113208        0.384906                   130.296   156.124\n",
       "    8        0.200843                    0.791061           2.46289   2.53648            0.916667         0.944056                    0.124528        0.509434                   146.289   153.648\n",
       "    9        0.300562                    0.489839           1.81642   2.29758            0.676056         0.85514                     0.181132        0.690566                   81.6423   129.758\n",
       "    10       0.400281                    0.232838           0.870369  1.94203            0.323944         0.722807                    0.0867925       0.777358                   -12.9631  94.2032\n",
       "    11       0.5                         0.0840864          0.946054  1.7434             0.352113         0.648876                    0.0943396       0.871698                   -5.39463  74.3396\n",
       "    12       0.599719                    0.0525291          0.264895  1.49756            0.0985915        0.557377                    0.0264151       0.898113                   -73.5105  49.7556\n",
       "    13       0.699438                    0.0374313          0.340579  1.33261            0.126761         0.495984                    0.0339623       0.932075                   -65.9421  33.2606\n",
       "    14       0.799157                    0.0275965          0.378421  1.21354            0.140845         0.45167                     0.0377358       0.969811                   -62.1579  21.3542\n",
       "    15       0.898876                    0.0159173          0.151369  1.09571            0.056338         0.407813                    0.0150943       0.984906                   -84.8631  9.57075\n",
       "    16       1                           0.000110079        0.149266  1                  0.0555556        0.372191                    0.0150943       1                          -85.0734  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>training_speed</b></td>\n",
       "<td><b>epochs</b></td>\n",
       "<td><b>iterations</b></td>\n",
       "<td><b>samples</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:37</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>None</td>\n",
       "<td>0.0</td>\n",
       "<td>0</td>\n",
       "<td>0.0</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:38</td>\n",
       "<td> 1.767 sec</td>\n",
       "<td>1363 obs/sec</td>\n",
       "<td>1.0</td>\n",
       "<td>1</td>\n",
       "<td>712.0</td>\n",
       "<td>0.4302337</td>\n",
       "<td>0.8656362</td>\n",
       "<td>0.8362796</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.2036517</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-04-06 14:32:40</td>\n",
       "<td> 3.666 sec</td>\n",
       "<td>3022 obs/sec</td>\n",
       "<td>10.0</td>\n",
       "<td>10</td>\n",
       "<td>7120.0</td>\n",
       "<td>0.3548854</td>\n",
       "<td>0.4226623</td>\n",
       "<td>0.8828880</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1587079</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    training_speed    epochs    iterations    samples    training_rmse    training_logloss    training_auc    training_lift    training_classification_error\n",
       "--  -------------------  ----------  ----------------  --------  ------------  ---------  ---------------  ------------------  --------------  ---------------  -------------------------------\n",
       "    2017-04-06 14:32:37  0.000 sec                     0         0             0          nan              nan                 nan             nan              nan\n",
       "    2017-04-06 14:32:38  1.767 sec   1363 obs/sec      1         1             712        0.430234         0.865636            0.83628         2.68679          0.203652\n",
       "    2017-04-06 14:32:40  3.666 sec   3022 obs/sec      10        10            7120       0.354885         0.422662            0.882888        2.68679          0.158708"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the DNN model summary\n",
    "dnn_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.1351007344471547\n",
      "RMSE: 0.3675605180744454\n",
      "LogLoss: 0.4699558932846622\n",
      "Mean Per-Class Error: 0.1787624140565317\n",
      "AUC: 0.881398013750955\n",
      "Gini: 0.7627960275019099\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24604683875781153: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>85.0</td>\n",
       "<td>17.0</td>\n",
       "<td>0.1667</td>\n",
       "<td> (17.0/102.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>15.0</td>\n",
       "<td>62.0</td>\n",
       "<td>0.1948</td>\n",
       "<td> (15.0/77.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>100.0</td>\n",
       "<td>79.0</td>\n",
       "<td>0.1788</td>\n",
       "<td> (32.0/179.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  ------------\n",
       "0      85   17   0.1667   (17.0/102.0)\n",
       "1      15   62   0.1948   (15.0/77.0)\n",
       "Total  100  79   0.1788   (32.0/179.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2460468</td>\n",
       "<td>0.7948718</td>\n",
       "<td>75.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0710559</td>\n",
       "<td>0.8616505</td>\n",
       "<td>99.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5401438</td>\n",
       "<td>0.8517350</td>\n",
       "<td>56.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5401438</td>\n",
       "<td>0.8379888</td>\n",
       "<td>56.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8062868</td>\n",
       "<td>0.9565217</td>\n",
       "<td>45.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0147199</td>\n",
       "<td>1.0</td>\n",
       "<td>151.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9978611</td>\n",
       "<td>0.9901961</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5401438</td>\n",
       "<td>0.6738336</td>\n",
       "<td>56.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2230665</td>\n",
       "<td>0.8137255</td>\n",
       "<td>78.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5401438</td>\n",
       "<td>0.8212376</td>\n",
       "<td>56.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.246047     0.794872  75\n",
       "max f2                       0.0710559    0.86165   99\n",
       "max f0point5                 0.540144     0.851735  56\n",
       "max accuracy                 0.540144     0.837989  56\n",
       "max precision                0.806287     0.956522  45\n",
       "max recall                   0.0147199    1         151\n",
       "max specificity              0.997861     0.990196  0\n",
       "max absolute_mcc             0.540144     0.673834  56\n",
       "max min_per_class_accuracy   0.223067     0.813725  78\n",
       "max mean_per_class_accuracy  0.540144     0.821238  56"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 43.02 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0111732</td>\n",
       "<td>0.9970545</td>\n",
       "<td>1.1623377</td>\n",
       "<td>1.1623377</td>\n",
       "<td>0.5</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.0129870</td>\n",
       "<td>16.2337662</td>\n",
       "<td>16.2337662</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0223464</td>\n",
       "<td>0.9966027</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.7435065</td>\n",
       "<td>1.0</td>\n",
       "<td>0.75</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0389610</td>\n",
       "<td>132.4675325</td>\n",
       "<td>74.3506494</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0335196</td>\n",
       "<td>0.9961337</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.9372294</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8333333</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0649351</td>\n",
       "<td>132.4675325</td>\n",
       "<td>93.7229437</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0446927</td>\n",
       "<td>0.9957089</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.0340909</td>\n",
       "<td>1.0</td>\n",
       "<td>0.875</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0909091</td>\n",
       "<td>132.4675325</td>\n",
       "<td>103.4090909</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0502793</td>\n",
       "<td>0.9934902</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.0663781</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.1038961</td>\n",
       "<td>132.4675325</td>\n",
       "<td>106.6378066</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1005587</td>\n",
       "<td>0.9759728</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.0663781</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.2077922</td>\n",
       "<td>106.6378066</td>\n",
       "<td>106.6378066</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1508380</td>\n",
       "<td>0.9322659</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.1524772</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9259259</td>\n",
       "<td>0.1168831</td>\n",
       "<td>0.3246753</td>\n",
       "<td>132.4675325</td>\n",
       "<td>115.2477152</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2011173</td>\n",
       "<td>0.8480251</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.1955267</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.1168831</td>\n",
       "<td>0.4415584</td>\n",
       "<td>132.4675325</td>\n",
       "<td>119.5526696</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3184358</td>\n",
       "<td>0.5719529</td>\n",
       "<td>1.8818800</td>\n",
       "<td>2.0799727</td>\n",
       "<td>0.8095238</td>\n",
       "<td>0.8947368</td>\n",
       "<td>0.2207792</td>\n",
       "<td>0.6623377</td>\n",
       "<td>88.1880025</td>\n",
       "<td>107.9972659</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4022346</td>\n",
       "<td>0.3334390</td>\n",
       "<td>0.9298701</td>\n",
       "<td>1.8403680</td>\n",
       "<td>0.4</td>\n",
       "<td>0.7916667</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.7402597</td>\n",
       "<td>-7.0129870</td>\n",
       "<td>84.0367965</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5027933</td>\n",
       "<td>0.1510848</td>\n",
       "<td>1.0331890</td>\n",
       "<td>1.6789322</td>\n",
       "<td>0.4444444</td>\n",
       "<td>0.7222222</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.8441558</td>\n",
       "<td>3.3189033</td>\n",
       "<td>67.8932179</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5977654</td>\n",
       "<td>0.0676966</td>\n",
       "<td>0.8204736</td>\n",
       "<td>1.5425416</td>\n",
       "<td>0.3529412</td>\n",
       "<td>0.6635514</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.9220779</td>\n",
       "<td>-17.9526356</td>\n",
       "<td>54.2541571</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7039106</td>\n",
       "<td>0.0395391</td>\n",
       "<td>0.1223513</td>\n",
       "<td>1.3283859</td>\n",
       "<td>0.0526316</td>\n",
       "<td>0.5714286</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.9350649</td>\n",
       "<td>-87.7648667</td>\n",
       "<td>32.8385900</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7988827</td>\n",
       "<td>0.0335244</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1704659</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5034965</td>\n",
       "<td>0.0</td>\n",
       "<td>0.9350649</td>\n",
       "<td>-100.0</td>\n",
       "<td>17.0465898</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9050279</td>\n",
       "<td>0.0172023</td>\n",
       "<td>0.4894053</td>\n",
       "<td>1.0905884</td>\n",
       "<td>0.2105263</td>\n",
       "<td>0.4691358</td>\n",
       "<td>0.0519481</td>\n",
       "<td>0.9870130</td>\n",
       "<td>-51.0594668</td>\n",
       "<td>9.0588424</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0011773</td>\n",
       "<td>0.1367456</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0588235</td>\n",
       "<td>0.4301676</td>\n",
       "<td>0.0129870</td>\n",
       "<td>1.0</td>\n",
       "<td>-86.3254393</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0111732                   0.997054           1.16234   1.16234            0.5              0.5                         0.012987        0.012987                   16.2338   16.2338\n",
       "    2        0.0223464                   0.996603           2.32468   1.74351            1                0.75                        0.025974        0.038961                   132.468   74.3506\n",
       "    3        0.0335196                   0.996134           2.32468   1.93723            1                0.833333                    0.025974        0.0649351                  132.468   93.7229\n",
       "    4        0.0446927                   0.995709           2.32468   2.03409            1                0.875                       0.025974        0.0909091                  132.468   103.409\n",
       "    5        0.0502793                   0.99349            2.32468   2.06638            1                0.888889                    0.012987        0.103896                   132.468   106.638\n",
       "    6        0.100559                    0.975973           2.06638   2.06638            0.888889         0.888889                    0.103896        0.207792                   106.638   106.638\n",
       "    7        0.150838                    0.932266           2.32468   2.15248            1                0.925926                    0.116883        0.324675                   132.468   115.248\n",
       "    8        0.201117                    0.848025           2.32468   2.19553            1                0.944444                    0.116883        0.441558                   132.468   119.553\n",
       "    9        0.318436                    0.571953           1.88188   2.07997            0.809524         0.894737                    0.220779        0.662338                   88.188    107.997\n",
       "    10       0.402235                    0.333439           0.92987   1.84037            0.4              0.791667                    0.0779221       0.74026                    -7.01299  84.0368\n",
       "    11       0.502793                    0.151085           1.03319   1.67893            0.444444         0.722222                    0.103896        0.844156                   3.3189    67.8932\n",
       "    12       0.597765                    0.0676966          0.820474  1.54254            0.352941         0.663551                    0.0779221       0.922078                   -17.9526  54.2542\n",
       "    13       0.703911                    0.0395391          0.122351  1.32839            0.0526316        0.571429                    0.012987        0.935065                   -87.7649  32.8386\n",
       "    14       0.798883                    0.0335244          0         1.17047            0                0.503497                    0               0.935065                   -100      17.0466\n",
       "    15       0.905028                    0.0172023          0.489405  1.09059            0.210526         0.469136                    0.0519481       0.987013                   -51.0595  9.05884\n",
       "    16       1                           0.00117731         0.136746  1                  0.0588235        0.430168                    0.012987        1                          -86.3254  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model performance on test dataset\n",
    "dnn_default.model_performance(titanic_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">      p0</th><th style=\"text-align: right;\">      p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.099139</td><td style=\"text-align: right;\">0.900861</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.110799</td><td style=\"text-align: right;\">0.889201</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.262639</td><td style=\"text-align: right;\">0.737361</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.789631</td><td style=\"text-align: right;\">0.210369</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.482829</td><td style=\"text-align: right;\">0.517171</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GLM model to make predictions\n",
    "yhat_test_glm = glm_default.predict(titanic_test)\n",
    "yhat_test_glm.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">         p0</th><th style=\"text-align: right;\">       p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.000425532</td><td style=\"text-align: right;\">0.999574 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0696667  </td><td style=\"text-align: right;\">0.930333 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.254742   </td><td style=\"text-align: right;\">0.745258 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.926995   </td><td style=\"text-align: right;\">0.0730054</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.442731   </td><td style=\"text-align: right;\">0.557269 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use DRF model to make predictions\n",
    "yhat_test_drf = drf_default.predict(titanic_test)\n",
    "yhat_test_drf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">       p0</th><th style=\"text-align: right;\">       p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0146597</td><td style=\"text-align: right;\">0.98534  </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0138361</td><td style=\"text-align: right;\">0.986164 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.117054 </td><td style=\"text-align: right;\">0.882946 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.922287 </td><td style=\"text-align: right;\">0.0777134</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.438084 </td><td style=\"text-align: right;\">0.561916 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GBM model to make predictions\n",
    "yhat_test_gbm = gbm_default.predict(titanic_test)\n",
    "yhat_test_gbm.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">       p0</th><th style=\"text-align: right;\">       p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0175347</td><td style=\"text-align: right;\">0.982465 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0419938</td><td style=\"text-align: right;\">0.958006 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.241484 </td><td style=\"text-align: right;\">0.758516 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.980339 </td><td style=\"text-align: right;\">0.0196613</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.621576 </td><td style=\"text-align: right;\">0.378424 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use DNN model to make predictions\n",
    "yhat_test_dnn = dnn_default.predict(titanic_test)\n",
    "yhat_test_dnn.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
